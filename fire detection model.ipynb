{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81467e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa15a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'D:/downloads/fire/fire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871b2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corrupt_images(directory):\n",
    "    corrupt_images = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    img.load()  # Fully load image\n",
    "                    img.convert(\"RGB\")  # Ensure format compatibility\n",
    "            except (IOError, SyntaxError, OSError, AttributeError) as e:\n",
    "                corrupt_images.append(file_path)\n",
    "                print(f\"❌ Corrupt image detected: {file_path} - {e}\")\n",
    "\n",
    "    # 🗑️ Delete corrupted images\n",
    "    for img in corrupt_images:\n",
    "        try:\n",
    "            os.remove(img)\n",
    "            print(f\"✅ Deleted: {img}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to delete {img}: {e}\")\n",
    "\n",
    "# 🔹 Run before training\n",
    "check_corrupt_images(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83713d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 20% validation split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a5afe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2805 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c81d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 700 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e60531b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ffe172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3), include_top=False, weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b198f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model(input_layer, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output_layer = layers.Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c809813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d4c0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e453ebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\python app\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 2s/step - accuracy: 0.9154 - loss: 0.3327 - val_accuracy: 0.9586 - val_loss: 0.1517\n",
      "Epoch 2/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - accuracy: 0.9616 - loss: 0.1206 - val_accuracy: 0.9286 - val_loss: 0.1821\n",
      "Epoch 3/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9621 - loss: 0.1305 - val_accuracy: 0.9186 - val_loss: 0.2070\n",
      "Epoch 4/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9619 - loss: 0.1075 - val_accuracy: 0.9186 - val_loss: 0.1953\n",
      "Epoch 5/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9643 - loss: 0.1029 - val_accuracy: 0.9257 - val_loss: 0.1933\n",
      "Epoch 6/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - accuracy: 0.9618 - loss: 0.1119 - val_accuracy: 0.9429 - val_loss: 0.1623\n",
      "Epoch 7/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.9566 - loss: 0.1288 - val_accuracy: 0.9071 - val_loss: 0.2063\n",
      "Epoch 8/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 0.9611 - loss: 0.0961 - val_accuracy: 0.9186 - val_loss: 0.2121\n",
      "Epoch 9/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.9657 - loss: 0.1133 - val_accuracy: 0.9043 - val_loss: 0.2224\n",
      "Epoch 10/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9707 - loss: 0.0972 - val_accuracy: 0.9186 - val_loss: 0.2120\n",
      "Epoch 11/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9701 - loss: 0.0892 - val_accuracy: 0.8386 - val_loss: 0.3126\n",
      "Epoch 12/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9691 - loss: 0.1005 - val_accuracy: 0.9014 - val_loss: 0.2446\n",
      "Epoch 13/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9670 - loss: 0.0976 - val_accuracy: 0.8900 - val_loss: 0.2328\n",
      "Epoch 14/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - accuracy: 0.9642 - loss: 0.0914 - val_accuracy: 0.9157 - val_loss: 0.2054\n",
      "Epoch 15/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9693 - loss: 0.0877 - val_accuracy: 0.8900 - val_loss: 0.2542\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b6a7142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8923 - loss: 0.2489\n",
      "✅ Validation Loss: 0.2288\n",
      "✅ Validation Accuracy: 0.8986\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f\"✅ Validation Loss: {loss:.4f}\")\n",
    "print(f\"✅ Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1688214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n"
     ]
    }
   ],
   "source": [
    "model.save(\"fire_detection.keras\", save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81cddb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Model Input Shape After Loading: (None, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model(\"fire_detection.keras\")\n",
    "print(\"📌 Model Input Shape After Loading:\", loaded_model.input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5117fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_img(img_path, target_size):\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=target_size)\n",
    "        return img.convert(\"RGB\")  # Ensure compatibility\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping unreadable image: {img_path}. Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "752eb7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 873ms/step\n",
      "🔍 Prediction Confidence: 0.7553\n",
      "🔥 Fire detected!\n"
     ]
    }
   ],
   "source": [
    "def predict_fire(img_path):\n",
    "    img = safe_load_img(img_path, target_size=(224, 224))\n",
    "    if img is None:\n",
    "        return  # Skip prediction if image is unreadable\n",
    "\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Convert to batch format\n",
    "    img_array /= 255.0  # Normalize\n",
    "\n",
    "    prediction = model.predict(img_array)[0][0]  # Get prediction value\n",
    "    print(f\"🔍 Prediction Confidence: {prediction:.4f}\")\n",
    "    \n",
    "    if prediction > 0.5:\n",
    "        print(\"🔥 Fire detected!\")\n",
    "    else:\n",
    "        print(\"✅ No Fire detected.\")\n",
    "\n",
    "# ✅ Test Prediction\n",
    "test_image_path = \"D:/downloads/fire/fire/val/images/1a228a4049efc30e.jpg\"\n",
    "predict_fire(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32751f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
