{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81467e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa15a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'D:/downloads/fire/fire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871b2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corrupt_images(directory):\n",
    "    corrupt_images = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    img.load()  # Fully load image\n",
    "                    img.convert(\"RGB\")  # Ensure format compatibility\n",
    "            except (IOError, SyntaxError, OSError, AttributeError) as e:\n",
    "                corrupt_images.append(file_path)\n",
    "                print(f\"âŒ Corrupt image detected: {file_path} - {e}\")\n",
    "\n",
    "    # ðŸ—‘ï¸ Delete corrupted images\n",
    "    for img in corrupt_images:\n",
    "        try:\n",
    "            os.remove(img)\n",
    "            print(f\"âœ… Deleted: {img}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to delete {img}: {e}\")\n",
    "\n",
    "# ðŸ”¹ Run before training\n",
    "check_corrupt_images(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83713d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 20% validation split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a5afe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2805 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c81d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 700 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e60531b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ffe172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3), include_top=False, weights='imagenet'\n",
    ")\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b198f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model(input_layer, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output_layer = layers.Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c809813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d4c0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e453ebd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\python app\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 2s/step - accuracy: 0.9154 - loss: 0.3327 - val_accuracy: 0.9586 - val_loss: 0.1517\n",
      "Epoch 2/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 1s/step - accuracy: 0.9616 - loss: 0.1206 - val_accuracy: 0.9286 - val_loss: 0.1821\n",
      "Epoch 3/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9621 - loss: 0.1305 - val_accuracy: 0.9186 - val_loss: 0.2070\n",
      "Epoch 4/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9619 - loss: 0.1075 - val_accuracy: 0.9186 - val_loss: 0.1953\n",
      "Epoch 5/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9643 - loss: 0.1029 - val_accuracy: 0.9257 - val_loss: 0.1933\n",
      "Epoch 6/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 1s/step - accuracy: 0.9618 - loss: 0.1119 - val_accuracy: 0.9429 - val_loss: 0.1623\n",
      "Epoch 7/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.9566 - loss: 0.1288 - val_accuracy: 0.9071 - val_loss: 0.2063\n",
      "Epoch 8/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 1s/step - accuracy: 0.9611 - loss: 0.0961 - val_accuracy: 0.9186 - val_loss: 0.2121\n",
      "Epoch 9/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 1s/step - accuracy: 0.9657 - loss: 0.1133 - val_accuracy: 0.9043 - val_loss: 0.2224\n",
      "Epoch 10/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9707 - loss: 0.0972 - val_accuracy: 0.9186 - val_loss: 0.2120\n",
      "Epoch 11/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9701 - loss: 0.0892 - val_accuracy: 0.8386 - val_loss: 0.3126\n",
      "Epoch 12/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9691 - loss: 0.1005 - val_accuracy: 0.9014 - val_loss: 0.2446\n",
      "Epoch 13/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.9670 - loss: 0.0976 - val_accuracy: 0.8900 - val_loss: 0.2328\n",
      "Epoch 14/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 1s/step - accuracy: 0.9642 - loss: 0.0914 - val_accuracy: 0.9157 - val_loss: 0.2054\n",
      "Epoch 15/15\n",
      "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - accuracy: 0.9693 - loss: 0.0877 - val_accuracy: 0.8900 - val_loss: 0.2542\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b6a7142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.8923 - loss: 0.2489\n",
      "âœ… Validation Loss: 0.2288\n",
      "âœ… Validation Accuracy: 0.8986\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f\"âœ… Validation Loss: {loss:.4f}\")\n",
    "print(f\"âœ… Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1688214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n"
     ]
    }
   ],
   "source": [
    "model.save(\"fire_detection.keras\", save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81cddb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Model Input Shape After Loading: (None, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model(\"fire_detection.keras\")\n",
    "print(\"ðŸ“Œ Model Input Shape After Loading:\", loaded_model.input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5117fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load_img(img_path, target_size):\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=target_size)\n",
    "        return img.convert(\"RGB\")  # Ensure compatibility\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Skipping unreadable image: {img_path}. Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "752eb7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 873ms/step\n",
      "ðŸ” Prediction Confidence: 0.7553\n",
      "ðŸ”¥ Fire detected!\n"
     ]
    }
   ],
   "source": [
    "def predict_fire(img_path):\n",
    "    img = safe_load_img(img_path, target_size=(224, 224))\n",
    "    if img is None:\n",
    "        return  # Skip prediction if image is unreadable\n",
    "\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Convert to batch format\n",
    "    img_array /= 255.0  # Normalize\n",
    "\n",
    "    prediction = model.predict(img_array)[0][0]  # Get prediction value\n",
    "    print(f\"ðŸ” Prediction Confidence: {prediction:.4f}\")\n",
    "    \n",
    "    if prediction > 0.5:\n",
    "        print(\"ðŸ”¥ Fire detected!\")\n",
    "    else:\n",
    "        print(\"âœ… No Fire detected.\")\n",
    "\n",
    "# âœ… Test Prediction\n",
    "test_image_path = \"D:/downloads/fire/fire/val/images/1a228a4049efc30e.jpg\"\n",
    "predict_fire(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32751f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
